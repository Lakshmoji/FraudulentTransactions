{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08873805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_amount transaction_type customer_address customer_email_domain  \\\n",
      "0          380.794718           mobile         Suburban             yahoo.com   \n",
      "1          951.207163         in-store            Urban             yahoo.com   \n",
      "2          734.674002           mobile         Suburban           outlook.com   \n",
      "3          602.671899           mobile            Rural           outlook.com   \n",
      "4          164.458454           online            Urban             gmail.com   \n",
      "\n",
      "  customer_bank_account    country  aml_risk_flag  is_fraud  \n",
      "0                Bank B  Country C              0         0  \n",
      "1                Bank A  Country B              0         0  \n",
      "2                Bank A  Country B              0         1  \n",
      "3                Bank A  Country C              0         0  \n",
      "4                Bank B  Country C              0         0  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DataGenerator:\n",
    "    def __init__(self, seed=42):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def generate_data(self, n_samples=1000):\n",
    "        data = {\n",
    "            'transaction_amount': np.random.uniform(10, 1000, n_samples),\n",
    "            'transaction_type': np.random.choice(['online', 'in-store', 'mobile'], n_samples),\n",
    "            'customer_address': np.random.choice(['Urban', 'Suburban', 'Rural'], n_samples),\n",
    "            'customer_email_domain': np.random.choice(['gmail.com', 'yahoo.com', 'outlook.com'], n_samples),\n",
    "            'customer_bank_account': np.random.choice(['Bank A', 'Bank B', 'Bank C'], n_samples),\n",
    "            'country': np.random.choice(['Country A', 'Country B', 'Country C'], n_samples),\n",
    "            'aml_risk_flag': np.random.choice([0, 1], n_samples, p=[0.9, 0.1]),\n",
    "            'is_fraud': np.random.choice([0, 1], n_samples, p=[0.95, 0.05])\n",
    "        }\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "data_generator = DataGenerator()\n",
    "df = data_generator.generate_data()\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b7c114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.numerical_cols = ['transaction_amount']\n",
    "        self.categorical_cols = ['transaction_type', 'customer_address', 'customer_bank_account', 'country', 'aml_risk_flag']\n",
    "        self.high_cardinality_cols = ['customer_email_domain']\n",
    "\n",
    "    def get_preprocessor(self):\n",
    "        numerical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())])\n",
    "\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numerical_transformer, self.numerical_cols),\n",
    "                ('cat', categorical_transformer, self.categorical_cols + self.high_cardinality_cols)])\n",
    "        return preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ced4bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       194\n",
      "           1       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.96       200\n",
      "   macro avg       0.48      0.49      0.49       200\n",
      "weighted avg       0.94      0.96      0.95       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.X = df.drop('is_fraud', axis=1)\n",
    "        self.y = df['is_fraud']\n",
    "        self.preprocessor = DataPreprocessor().get_preprocessor()\n",
    "\n",
    "    def train_model(self):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Create a preprocessing and modeling pipeline\n",
    "        model = Pipeline(steps=[('preprocessor', self.preprocessor),\n",
    "                                ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "        # Define a grid of hyperparameters\n",
    "        param_grid = {\n",
    "            'classifier__n_estimators': [100, 200],\n",
    "            'classifier__max_depth': [None, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 5],\n",
    "            'classifier__min_samples_leaf': [1, 2]\n",
    "        }\n",
    "\n",
    "        # Set up the grid search\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        best_params = grid_search.best_params_\n",
    "        best_score = grid_search.best_score_\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Predictions and evaluation\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        \n",
    "        # Feature importances (assuming the best model is a RandomForest)\n",
    "        feature_importances = best_model.named_steps['classifier'].feature_importances_\n",
    "        \n",
    "        return best_params, best_score, report, feature_importances\n",
    "\n",
    "model_trainer = ModelTrainer(df)\n",
    "best_params, best_score, report, feature_importances = model_trainer.train_model()\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07def94b",
   "metadata": {},
   "source": [
    "# Conformal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nonconformist.cp import IcpClassifier\n",
    "from nonconformist.nc import NcFactory\n",
    "\n",
    "# Create a nonconformity function based on the RandomForest model\n",
    "nc = NcFactory.create_nc(clf)\n",
    "\n",
    "# Initialize the Inductive Conformal Predictor\n",
    "icp = IcpClassifier(nc)\n",
    "\n",
    "# Fit the ICP using the training data\n",
    "icp.fit(X_train, y_train)\n",
    "\n",
    "# Calibrate the ICP with a separate calibration dataset if available\n",
    "# For simplicity, we use the test set here, but it's better to use a separate calibration set\n",
    "icp.calibrate(X_test, y_test)\n",
    "\n",
    "# Make predictions with confidence and credibility intervals\n",
    "prediction_intervals = icp.predict(X_test, significance=0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60935115",
   "metadata": {},
   "source": [
    "The prediction_intervals will contain the prediction regions for each instance in X_test at the specified significance level (e.g., 0.05 for 95% confidence). For each instance, you'll get a set of possible labels (in this binary classification case, fraudulent or not) along with the measure of confidence (how confident the model is about the prediction) and credibility (how likely the prediction is correct given the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e208b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example evaluation code\n",
    "correct = 0\n",
    "for i, interval in enumerate(prediction_intervals):\n",
    "    if y_test.iloc[i] in interval:\n",
    "        correct += 1\n",
    "accuracy = correct / len(y_test)\n",
    "print(f\"Conformal prediction accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
